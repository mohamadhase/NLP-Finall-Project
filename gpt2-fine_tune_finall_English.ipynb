{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center;font-size:40pt\"> imports </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nasser\\.conda\\envs\\finall\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import TFGPT2LMHeadModel, GPT2Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import pandas as pd"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center;font-size:40pt\"> Load PreTrianed Model </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFGPT2LMHeadModel: ['transformer.h.11.attn.masked_bias', 'lm_head.weight', 'transformer.h.0.attn.masked_bias', 'transformer.h.10.attn.masked_bias', 'transformer.h.8.attn.masked_bias', 'transformer.h.7.attn.masked_bias', 'transformer.h.9.attn.masked_bias', 'transformer.h.3.attn.masked_bias', 'transformer.h.5.attn.masked_bias', 'transformer.h.1.attn.masked_bias', 'transformer.h.6.attn.masked_bias', 'transformer.h.2.attn.masked_bias', 'transformer.h.4.attn.masked_bias']\n",
      "- This IS expected if you are initializing TFGPT2LMHeadModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFGPT2LMHeadModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFGPT2LMHeadModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "# Load pretrained BART model and tokenizer\n",
    "model_name = 'ismaelfaro/gpt2-poems.en'\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = TFGPT2LMHeadModel.from_pretrained(model_name,from_pt=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center;font-size:40pt\"> Load Dataset </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('merged_data.csv')\n",
    "df = df.sample(frac=1)\n",
    "df = df.sample(100)\n",
    "lines = df['Verse'].values.tolist()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center;font-size:40pt\"> Tokenize the Dataset </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max-len = 14\n"
     ]
    }
   ],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "max_length = max([len(tokenizer.encode(line)) for line in lines])\n",
    "print(f\"max-len = {max_length}\")\n",
    "# Tokenize the lines\n",
    "tokenized_lines = tokenizer(\n",
    "    lines,\n",
    "    truncation=True,\n",
    "    padding='max_length',\n",
    "    max_length=max_length,\n",
    "    add_special_tokens=True\n",
    ")['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original line: a good man seen though silent counsel gives\n",
      "tokenized line: [64, 922, 582, 1775, 996, 10574, 7739, 3607, 50256, 50256, 50256, 50256, 50256, 50256]\n"
     ]
    }
   ],
   "source": [
    "print(f\"original line: {lines[0]}\")\n",
    "print(f\"tokenized line: {tokenized_lines[0]}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center;font-size:40pt\"> Create Sequences and labels  </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten the tokenized lines\n",
    "input_sequences = [line[:-1] for line in tokenized_lines]\n",
    "labels = [line[1:] for line in tokenized_lines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original line: a good man seen though silent counsel gives\n",
      "input sequence : [64, 922, 582, 1775, 996, 10574, 7739, 3607, 50256, 50256, 50256, 50256, 50256]\n",
      "input sequence as text: a good man seen though silent counsel gives<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n",
      "label: [922, 582, 1775, 996, 10574, 7739, 3607, 50256, 50256, 50256, 50256, 50256, 50256]\n",
      "label as text:  good man seen though silent counsel gives<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "print(f\"original line: {lines[0]}\")\n",
    "print(f\"input sequence : {input_sequences[0]}\")\n",
    "print(f\"input sequence as text: {tokenizer.decode(input_sequences[0])}\")\n",
    "print(f\"label: {labels[0]}\")\n",
    "print(f\"label as text: {tokenizer.decode(labels[0])}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center;font-size:40pt\"> Compile the model </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tuning parameters\n",
    "epochs = 5\n",
    "learning_rate = 1e-5\n",
    "# Compile the model\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center;font-size:40pt\"> Train the model </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "13/13 [==============================] - 16s 95ms/step - loss: 10.9695\n",
      "Epoch 2/5\n",
      "13/13 [==============================] - 1s 94ms/step - loss: 6.8196\n",
      "Epoch 3/5\n",
      "13/13 [==============================] - 1s 93ms/step - loss: 5.8319\n",
      "Epoch 4/5\n",
      "13/13 [==============================] - 1s 94ms/step - loss: 5.3509\n",
      "Epoch 5/5\n",
      "13/13 [==============================] - 1s 94ms/step - loss: 5.0655\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2be8debb788>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fine-tuning\n",
    "model.fit(input_sequences,labels,batch_size = 8, epochs=epochs)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center;font-size:40pt\"> Test the model output </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sadnessIs dilapidationDespite your rush, despite your witThe weightless whisper of the snail, The powerless thought of the coal, The lossless peace of the ocean breeze, The hopeless weightless agony of your soul on this stone yo\n",
      "west.By Sidney \n",
      " July 15,2008so often my sadness sends you to sleep  \\nso very often my fingers walk over you with tears  \\nyour eyes so gently squeezed as the sun remembers your childhood  \\nI\n"
     ]
    }
   ],
   "source": [
    "seed_test = \"sadness\"\n",
    "input_ids = tokenizer.encode(seed_test, return_tensors='tf')\n",
    "\n",
    "sample_outputs = model.generate(\n",
    "    input_ids, # The input sequence encoded as token IDs.\n",
    "    do_sample=True,\n",
    "    max_length=100,  # The maximum length of the generated output.\n",
    "    top_k=0,\n",
    "    top_p=0.9,\n",
    "    temperature=1,\n",
    "    num_return_sequences=1,\n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "\n",
    "output = tokenizer.decode(sample_outputs[0], skip_special_tokens=True)\n",
    "output = output.replace('-', '\\n')\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create validation sequences\n",
    "df = pd.read_csv('merged_data.csv')\n",
    "df = df.sample(frac=1)\n",
    "df = df.sample(2000)\n",
    "lines = df['Verse'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max-len = 18\n"
     ]
    }
   ],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "max_length = max([len(tokenizer.encode(line)) for line in lines])\n",
    "print(f\"max-len = {max_length}\")\n",
    "# Tokenize the lines\n",
    "tokenized_lines = tokenizer(\n",
    "    lines,\n",
    "    truncation=True,\n",
    "    padding='max_length',\n",
    "    max_length=max_length,\n",
    "    add_special_tokens=True\n",
    ")['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten the tokenized lines\n",
    "validation_sequences = [line[:-1] for line in tokenized_lines]\n",
    "validation_labels = [line[1:] for line in tokenized_lines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: 22.884700775146484\n"
     ]
    }
   ],
   "source": [
    "# Calculate perplexity\n",
    "loss = model.evaluate(validation_sequences, validation_labels, verbose=0)\n",
    "perplexity = tf.exp(loss)\n",
    "print(f\"Perplexity: {perplexity}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('models/english/gpt2-poems-tokenizer.en\\\\tokenizer_config.json',\n",
       " 'models/english/gpt2-poems-tokenizer.en\\\\special_tokens_map.json',\n",
       " 'models/english/gpt2-poems-tokenizer.en\\\\vocab.json',\n",
       " 'models/english/gpt2-poems-tokenizer.en\\\\merges.txt',\n",
       " 'models/english/gpt2-poems-tokenizer.en\\\\added_tokens.json')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # save model\n",
    "# model.save_pretrained('models/english/gpt2-poems.en')\n",
    "# # save tokenizer\n",
    "# tokenizer.save_pretrained('models/english/gpt2-poems-tokenizer.en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "finall",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
